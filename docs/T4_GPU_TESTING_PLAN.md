# T4 GPU Testing Plan for SAP HANA Cloud LangChain Integration

This document outlines a comprehensive testing plan for the SAP HANA Cloud LangChain integration deployed on an NVIDIA T4 GPU instance in Brev Cloud.

## Prerequisites

- Access to the deployed Jupyter Lab instance: `https://jupyter0-513syzm60.brevlab.com`
- NVIDIA Enterprise authentication credentials
- Sample documents and test queries (generated using the provided `create_test_data.py` script)

## Testing Areas

### 1. Environment Verification

- [ ] Verify Jupyter Lab accessibility
- [ ] Confirm NVIDIA T4 GPU is properly detected
- [ ] Check CUDA version and drivers
- [ ] Validate Python environment and installed packages
- [ ] Verify SAP HANA Cloud connection

#### Commands to Run:

```python
# Check GPU visibility
!nvidia-smi

# Check CUDA version
!nvcc --version

# Check installed packages
!pip list | grep -E "langchain|hdbcli|torch|numpy|sentence-transformers"

# Check Python environment
import sys
print(f"Python version: {sys.version}")
```

### 2. TensorRT Optimization Tests

- [ ] Verify TensorRT engine creation
- [ ] Test embedding generation with TensorRT optimization
- [ ] Compare TensorRT vs. PyTorch performance
- [ ] Test different precision modes (FP16, FP32)

#### Testing Approach:

1. Create a notebook to test TensorRT engine initialization
2. Generate embeddings for sample documents with both TensorRT and PyTorch
3. Measure and compare performance metrics
4. Test with different batch sizes to find optimal settings for T4 GPU

### 3. Core Functionality Tests

- [ ] Test vector store creation
- [ ] Test document addition with embeddings
- [ ] Test similarity search
- [ ] Test filtered search
- [ ] Test MMR search
- [ ] Test SPARQL graph queries (if applicable)

#### Testing Approach:

1. Create a test notebook for each functional area
2. Use the sample documents generated by the test data script
3. Verify results match expected patterns
4. Log performance metrics for each operation

### 4. GPU Acceleration Tests

- [ ] Test embedding generation performance with T4 GPU
- [ ] Benchmark vector operations performance
- [ ] Test maximum batch size optimization
- [ ] Measure memory usage patterns
- [ ] Test fallback mechanisms when GPU memory is exhausted

#### Testing Approach:

1. Create a dedicated performance benchmark notebook
2. Test with progressively larger batch sizes (1, 8, 16, 32, 64, 128)
3. Measure latency and throughput
4. Monitor GPU memory usage during operations
5. Find optimal settings for T4 GPU's 16GB memory

### 5. Multi-Backend Deployment Tests

- [ ] Test configuration switching for different environments
- [ ] Validate environment-specific settings
- [ ] Test platform-specific optimizations

### 6. Error Handling and Recovery Tests

- [ ] Test behavior when GPU memory is exhausted
- [ ] Test database connection error handling
- [ ] Test graceful degradation to CPU when needed
- [ ] Verify error messages are helpful and context-aware

## Performance Benchmarking

For each test, collect the following metrics:

1. **Latency**: Time to complete operations
2. **Throughput**: Operations per second
3. **Memory Usage**: Peak memory consumption
4. **GPU Utilization**: Percentage of GPU compute resources used
5. **CPU vs. GPU Speedup**: Comparative performance gains

Create performance comparison charts for:
- Embedding generation (various batch sizes)
- Similarity search (with and without filtering)
- MMR search performance
- End-to-end RAG pipeline performance

## Test Notebook Templates

### 1. Environment Verification Notebook

```python
# Environment Verification
!nvidia-smi
!nvcc --version
!pip list | grep -E "langchain|hdbcli|torch|numpy|sentence-transformers"

import sys
import torch
print(f"Python version: {sys.version}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA device count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"CUDA device name: {torch.cuda.get_device_name(0)}")
    print(f"CUDA device capability: {torch.cuda.get_device_capability(0)}")
```

### 2. TensorRT Optimization Test Notebook

```python
# TensorRT Optimization Test
from langchain_hana.gpu.tensorrt_embeddings import TensorRTEmbeddings
from langchain_hana.gpu.utils import get_optimal_batch_size
import time
import torch
from sentence_transformers import SentenceTransformer
import numpy as np

# Load test documents
import json
with open('test_data/sample_documents.json', 'r') as f:
    documents = json.load(f)

texts = [doc["page_content"] for doc in documents]

# Test TensorRT optimized embeddings
tensorrt_embeddings = TensorRTEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    precision="fp16"  # Use FP16 for T4 GPU
)

# Test PyTorch embeddings for comparison
pytorch_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
pytorch_model = pytorch_model.to("cuda" if torch.cuda.is_available() else "cpu")

# Benchmark functions
def benchmark_tensorrt(texts, batch_size=32):
    start_time = time.time()
    embeddings = tensorrt_embeddings.embed_documents(texts)
    end_time = time.time()
    return embeddings, (end_time - start_time) * 1000  # ms

def benchmark_pytorch(texts, batch_size=32):
    start_time = time.time()
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        with torch.no_grad():
            batch_embeddings = pytorch_model.encode(batch)
        embeddings.extend(batch_embeddings)
    end_time = time.time()
    return embeddings, (end_time - start_time) * 1000  # ms

# Run benchmarks for different batch sizes
batch_sizes = [1, 8, 32, 64, 128]
results = {}

for batch_size in batch_sizes:
    print(f"Testing batch size: {batch_size}")
    
    # TensorRT test
    tensorrt_embeddings, tensorrt_time = benchmark_tensorrt(texts, batch_size)
    
    # PyTorch test
    pytorch_embeddings, pytorch_time = benchmark_pytorch(texts, batch_size)
    
    # Compare results
    results[batch_size] = {
        "tensorrt_time_ms": tensorrt_time,
        "pytorch_time_ms": pytorch_time,
        "speedup": pytorch_time / tensorrt_time
    }
    
    print(f"TensorRT time: {tensorrt_time:.2f} ms")
    print(f"PyTorch time: {pytorch_time:.2f} ms")
    print(f"Speedup: {pytorch_time / tensorrt_time:.2f}x")
    
    # Verify embedding quality
    if batch_size == 1:  # Only check once
        tensorrt_embedding = np.array(tensorrt_embeddings[0])
        pytorch_embedding = np.array(pytorch_embeddings[0])
        cosine_similarity = np.dot(tensorrt_embedding, pytorch_embedding) / (
            np.linalg.norm(tensorrt_embedding) * np.linalg.norm(pytorch_embedding)
        )
        print(f"Embedding similarity: {cosine_similarity:.4f}")
```

### 3. Vector Store Functionality Test Notebook

```python
# Vector Store Functionality Test
from langchain_hana.vectorstores import HanaDB
from langchain_hana.gpu.tensorrt_embeddings import TensorRTEmbeddings
from hdbcli import dbapi
import time
import json

# Load test documents and queries
with open('test_data/sample_documents.json', 'r') as f:
    documents = json.load(f)

with open('test_data/test_queries.json', 'r') as f:
    queries = json.load(f)

# Connect to SAP HANA Cloud
connection = dbapi.connect(
    address="<hostname>",
    port=<port>,
    user="<username>",
    password="<password>",
    encrypt=True,
    sslValidateCertificate=True
)

# Initialize TensorRT embeddings
embeddings = TensorRTEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    precision="fp16"  # Use FP16 for T4 GPU
)

# Create vector store
vectorstore = HanaDB(
    connection=connection,
    embedding=embeddings,
    table_name="T4_TEST_VECTORS"
)

# Add documents
start_time = time.time()
texts = [doc["page_content"] for doc in documents]
metadatas = [doc["metadata"] for doc in documents]
vectorstore.add_texts(texts=texts, metadatas=metadatas)
add_time = time.time() - start_time
print(f"Added {len(texts)} documents in {add_time:.2f} seconds")

# Create HNSW index for performance
vectorstore.create_hnsw_index(
    m=64,
    ef_construction=128,
    ef_search=64
)

# Test similarity search
for query in queries[:3]:  # Test first 3 queries
    print(f"\nTesting query: {query['text']}")
    start_time = time.time()
    
    if query["options"].get("filter"):
        if query["options"].get("use_mmr"):
            results = vectorstore.max_marginal_relevance_search(
                query["text"],
                k=query["options"]["k"],
                lambda_mult=query["options"].get("lambda_mult", 0.5),
                filter=query["options"]["filter"]
            )
        else:
            results = vectorstore.similarity_search(
                query["text"],
                k=query["options"]["k"],
                filter=query["options"]["filter"]
            )
    else:
        if query["options"].get("use_mmr"):
            results = vectorstore.max_marginal_relevance_search(
                query["text"],
                k=query["options"]["k"],
                lambda_mult=query["options"].get("lambda_mult", 0.5)
            )
        else:
            results = vectorstore.similarity_search(
                query["text"],
                k=query["options"]["k"]
            )
    
    search_time = time.time() - start_time
    print(f"Search completed in {search_time*1000:.2f} ms")
    print(f"Found {len(results)} results")
    
    # Print first result
    if results:
        print(f"Top result: {results[0].page_content[:100]}...")
        print(f"Metadata: {results[0].metadata}")
```

## Expected Outcomes

1. Successful verification of T4 GPU deployment
2. Performance benchmarks showing significant speedup with GPU acceleration
3. Identification of optimal batch sizes and configuration settings for T4 GPU
4. Confirmation of all functional requirements
5. Documentation of any issues or limitations

## Reporting

Create a comprehensive test report that includes:

1. Test environment details
2. Test results for each test area
3. Performance metrics and comparisons
4. Visualizations of key performance indicators
5. Recommendations for production deployment
6. Any issues or limitations encountered

## Next Steps After Testing

1. Optimize configuration based on test results
2. Document best practices for T4 GPU deployment
3. Create deployment guides for different environments
4. Implement any fixes for identified issues