name: finsightutils-langchain-for-sap-hana-cloud
version: 1.0.0
description: SAP HANA Cloud LangChain Integration with NVIDIA GPU acceleration and TensorRT optimization

# Blueprint metadata
author: plturrell
website: https://github.com/plturrell/finsightutils_langchain_for_sap_hana_cloud
license: Apache-2.0
documentation: https://github.com/plturrell/finsightutils_langchain_for_sap_hana_cloud/blob/main/README.md
category: AI/ML
tags:
  - sap
  - hana
  - langchain
  - vectorstore
  - gpu
  - tensorrt
  - nvidia

# System requirements
hardware:
  gpu: required
  gpu_type: nvidia-t4
  min_gpu_count: 1
  gpu_memory: 16GB
  memory: 32GB
  cpu_cores: 8
  storage: 50GB

# Blueprint configuration
configuration:
  environment_variables:
    - name: HANA_HOST
      description: SAP HANA Cloud host
      required: true
    - name: HANA_PORT
      description: SAP HANA Cloud port
      default: "443"
    - name: HANA_USER
      description: SAP HANA Cloud username
      required: true
    - name: HANA_PASSWORD
      description: SAP HANA Cloud password
      required: true
      secret: true
    - name: DEFAULT_TABLE_NAME
      description: Default table for vector store
      default: "EMBEDDINGS"
    - name: GPU_ENABLED
      default: "true"
    - name: USE_TENSORRT
      default: "true"
    - name: TENSORRT_PRECISION
      default: "fp16"
    - name: BATCH_SIZE
      default: "32"
    - name: MAX_BATCH_SIZE
      default: "128"
    - name: ENABLE_MULTI_GPU
      default: "true"
    - name: GPU_MEMORY_FRACTION
      default: "0.9"
    - name: PORT
      default: "8000"
    - name: LOG_LEVEL
      default: "INFO"
    - name: ENABLE_CORS
      description: "Enable CORS for API requests"
      default: "true"
    - name: CORS_ORIGINS
      description: "Allowed CORS origins (comma-separated list)"
      default: "https://example.com,http://localhost:3000"
    - name: JWT_SECRET
      description: "JWT authentication secret key"
      required: true
      secret: true
    - name: DALI_ENABLED
      description: "Enable NVIDIA DALI for accelerated data loading"
      default: "true" 
    - name: USE_TRANSFORMER_ENGINE
      description: "Enable NVIDIA Transformer Engine optimizations"
      default: "true"
    - name: NVTX_PROFILING_ENABLED
      description: "Enable NVTX profiling markers"
      default: "true"
    - name: AUTO_TUNE_ENABLED
      description: "Enable automatic tuning of parameters"
      default: "true"
    - name: AUTO_TUNE_DURATION_MINUTES
      description: "Duration for auto-tuning in minutes"
      default: "60"

  ports:
    - port: 8000
      protocol: http
      description: API service
    - port: 3000
      protocol: http
      description: Frontend service
    - port: 9400
      protocol: http
      description: DCGM exporter metrics
    - port: 9090
      protocol: http
      description: Prometheus metrics
    - port: 8001
      protocol: http
      description: Triton Server HTTP endpoint
    - port: 8002
      protocol: http
      description: Triton Server metrics endpoint

# Installation and setup
installation:
  repository:
    url: https://github.com/plturrell/finsightutils_langchain_for_sap_hana_cloud.git
    branch: nvidia-vercel-deployment
  
  initialization:
    script: .brev/startup.sh
  
  dependencies:
    python:
      - requirements.txt
      - requirements-monitoring.txt
    apt:
      - curl
      - wget
      - git
      - nvidia-container-toolkit
      - pciutils

# Services
services:
  - name: api
    type: web
    command: "python -m uvicorn api.core.main:app --host 0.0.0.0 --port 8000 --workers 4"
    port: 8000
    health_check:
      path: /health/ping
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - name: model-repository
        mountPath: /app/models
      - name: trt-engines
        mountPath: /app/trt_engines
      - name: calibration-cache
        mountPath: /app/calibration_cache
    security_context:
      run_as_non_root: true
      read_only_root_filesystem: false
    replicas: 2
    autoscaling:
      min_replicas: 2
      max_replicas: 5
      metrics:
        - type: Resource
          resource:
            name: cpu
            target_utilization_percentage: 80
    restart_policy:
      condition: any
      delay: 5s
      max_attempts: 3
  
  - name: frontend
    type: web
    command: "cd frontend && npm start"
    port: 3000
    depends_on:
      - api
    security_context:
      run_as_non_root: true
      read_only_root_filesystem: true

  - name: triton-server
    type: service
    image: nvcr.io/nvidia/tritonserver:22.12-py3
    ports:
      - port: 8000
        target_port: 8000
      - port: 8001
        target_port: 8001
      - port: 8002
        target_port: 8002
    volumes:
      - name: model-repository
        mountPath: /models
    command: "tritonserver --model-repository=/models --strict-model-config=false --log-verbose=1"
    resources:
      gpu: true
    health_check:
      path: /v2/health/ready
      port: 8000
      interval: 30s
      timeout: 10s
      retries: 3

  - name: dcgm-exporter
    type: service
    image: nvcr.io/nvidia/k8s/dcgm-exporter:2.4.6-2.6.10-ubuntu20.04
    ports:
      - port: 9400
        target_port: 9400
    volumes:
      - name: dcgm-config
        mountPath: /etc/dcgm-exporter
    command: "dcgm-exporter -f /etc/dcgm-exporter/default-counters.csv"

  - name: prometheus
    type: service
    image: prom/prometheus:v2.40.1
    ports:
      - port: 9090
        target_port: 9090
    volumes:
      - name: prometheus-config
        mountPath: /etc/prometheus
    command: "--config.file=/etc/prometheus/prometheus.yml"
    depends_on:
      - dcgm-exporter
      - api

# Volumes
volumes:
  - name: model-repository
    size: 10Gi
    persistent: true
  - name: trt-engines
    size: 5Gi
    persistent: true
  - name: calibration-cache
    size: 2Gi
    persistent: true
  - name: dcgm-config
    size: 1Gi
    persistent: true
  - name: prometheus-config
    size: 1Gi
    persistent: true

# Post-installation checks
checks:
  - name: API health check
    command: "curl -f http://localhost:8000/health/ping"
    timeout: 10s
    retries: 3
  
  - name: GPU acceleration check
    command: "curl -f http://localhost:8000/gpu/info"
    timeout: 10s
    retries: 3

  - name: Triton Server health check
    command: "curl -f http://localhost:8001/v2/health/ready"
    timeout: 10s
    retries: 3

  - name: DCGM metrics check
    command: "curl -f http://localhost:9400/metrics | grep -q DCGM_FI_DEV_GPU_UTIL"
    timeout: 10s
    retries: 3

  - name: Container security scan
    command: "trivy image --severity HIGH,CRITICAL --exit-code 1 ${SERVICE_IMAGE}"
    on: pre-deploy
    timeout: 300s

# Setup scripts
setup:
  - name: Install NVIDIA DALI
    run: |
      pip install --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda110

  - name: Optimize models with TensorRT
    run: |
      python -m scripts.optimize_models \
        --precision=${TENSORRT_PRECISION} \
        --batch-sizes=1,2,4,8,16,32,64,128 \
        --calibration-cache=/app/calibration_cache \
        --export-format=tensorrt,onnx

  - name: Configure Prometheus
    run: |
      mkdir -p /etc/prometheus
      cat > /etc/prometheus/prometheus.yml << EOF
      global:
        scrape_interval: 15s
        evaluation_interval: 15s
      scrape_configs:
        - job_name: 'dcgm'
          static_configs:
            - targets: ['dcgm-exporter:9400']
        - job_name: 'api'
          static_configs:
            - targets: ['api:8000']
        - job_name: 'triton'
          static_configs:
            - targets: ['triton-server:8002']
      EOF

  - name: Configure DCGM Exporter
    run: |
      mkdir -p /etc/dcgm-exporter
      cat > /etc/dcgm-exporter/default-counters.csv << EOF
      # Format,,
      # DCGM FI Field ID, Prometheus metric type, help message
      
      # GPU utilization
      DCGM_FI_DEV_GPU_UTIL, gauge, GPU utilization (in %)
      
      # Memory utilization
      DCGM_FI_DEV_FB_USED, gauge, GPU framebuffer memory used (in MiB)
      DCGM_FI_DEV_FB_FREE, gauge, GPU framebuffer memory free (in MiB)
      DCGM_FI_DEV_FB_TOTAL, gauge, GPU framebuffer memory total (in MiB)
      
      # SM clocks
      DCGM_FI_DEV_SM_CLOCK, gauge, SM clock frequency (in MHz)
      
      # Memory clocks
      DCGM_FI_DEV_MEM_CLOCK, gauge, Memory clock frequency (in MHz)
      
      # Power usage
      DCGM_FI_DEV_POWER_USAGE, gauge, Power usage (in W)
      
      # Temperature
      DCGM_FI_DEV_GPU_TEMP, gauge, GPU temperature (in C)
      
      # PCIe throughput
      DCGM_FI_DEV_PCIE_TX_THROUGHPUT, gauge, PCIe transmit throughput (in KB/s)
      DCGM_FI_DEV_PCIE_RX_THROUGHPUT, gauge, PCIe receive throughput (in KB/s)
      EOF

# Additional documentation
documentation:
  short_description: |
    GPU-accelerated vector search for SAP HANA Cloud using LangChain with NVIDIA TensorRT optimization.
  
  usage_example: |
    # Connect to the API
    BASE_URL="http://localhost:8000"
    
    # Add texts to the vector store
    curl -X POST "$BASE_URL/texts" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $JWT_TOKEN" \
      -d '{"texts": ["This is a sample text", "Another example text"], "metadatas": [{"source": "example"}, {"source": "example"}]}'
    
    # Query the vector store
    curl -X POST "$BASE_URL/query" \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $JWT_TOKEN" \
      -d '{"query": "sample", "k": 2}'
    
    # Get GPU information
    curl "$BASE_URL/gpu/info" \
      -H "Authorization: Bearer $JWT_TOKEN"
    
    # Access monitoring dashboards
    echo "Prometheus: http://localhost:9090"
    echo "DCGM Metrics: http://localhost:9400/metrics"
    echo "Triton Metrics: http://localhost:8002/metrics"
  
  additional_resources:
    - title: GitHub Repository
      url: https://github.com/plturrell/finsightutils_langchain_for_sap_hana_cloud
    - title: SAP HANA Cloud Documentation
      url: https://help.sap.com/docs/HANA_CLOUD
    - title: LangChain Documentation
      url: https://python.langchain.com/docs/get_started
    - title: NVIDIA TensorRT Documentation
      url: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html
    - title: NVIDIA Triton Server Documentation
      url: https://github.com/triton-inference-server/server
    - title: NVIDIA DCGM Documentation
      url: https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/index.html